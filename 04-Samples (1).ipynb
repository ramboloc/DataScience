{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<hr/>\n",
    "\n",
    "# Introduction to Data Science\n",
    "**Tamás Budavári** - budavari@jhu.edu <br/>\n",
    "\n",
    "- Sampling from distributions\n",
    "- Density estimation\n",
    "\n",
    "<hr/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1><font color=\"darkblue\">Sampling and Density Estimation</font></h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Descriptive Statistics\n",
    "- Characterization of location, dispersion, etc.\n",
    "\n",
    "| | Sample Estimates <font color=\"white\">(notations)</font> | Probabilisty Density Functions   |\n",
    "|--|--------------|-------------|\n",
    "| **Average** | $\\displaystyle\\ \\bar{x}=\\frac{1}{N}\\sum_{i=1}^N x_i = \\big\\langle x_i \\big\\rangle_{i=1}^N$ | $\\displaystyle\\ \\mu = \\mathbb{E}[X] =\\!\\int\\!x\\,p(x)\\,dx$| \n",
    "| **Variance** | $\\displaystyle\\ s^2=\\frac{1}{N\\!-\\!1}\\sum_{i=1}^N \\big(x_i\\!-\\!\\bar{x}\\big)^2 $|  $\\displaystyle\\ \\mathbb{Var}[X] =\\!\\int\\!(x\\!-\\!\\mu)^2 p(x)\\,dx$| \n",
    "\n",
    "- Useful connection to sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Sampling from distributions\n",
    "\n",
    "- Uniform between $a$ and $b$: scale and shift\n",
    "\n",
    ">$\\displaystyle U_{ab} = a + (b\\!-\\!a)\\,U_{01} $\n",
    "\n",
    "- Inverse transform sampling in $\\mathbb{R}$\n",
    "\n",
    ">$\\displaystyle X = \\mathrm{CDF}^{-1}(U_{01}) $\n",
    "><br/>\n",
    "> Unhomework: prove it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"files/inv.png\" height=\"400\" width=\"400\" align=left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- Rejection sampling - also works in $\\mathbb{R}^N$\n",
    "\n",
    "<img src=\"files/anim.gif\" align=left>\n",
    "<!--<img src=\"http://dl.dropbox.com/u/27415200/anim.gif\">-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "### Numerical Methods\n",
    "If the $\\left\\{x_i\\right\\}$ set is sampled from the probability density function $p(\\cdot)$,\n",
    "the following will be true:\n",
    "- Average\n",
    "\n",
    ">$\\displaystyle\\mathbb{E}[X] =\\!\\int x\\ p(x)\\,dx  \\ \\approx\\ \\frac{1}{N}\\sum_i x_i $\n",
    "\n",
    "> Or a function of $x$\n",
    ">\n",
    ">$\\displaystyle\\mathbb{E}[f(X)] =\\!\\int f(x)\\ p(x)\\,dx  \\ \\approx\\ \\frac{1}{N}\\sum_i f(x_i) $\n",
    "\n",
    "- Variance\n",
    "\n",
    ">$\\displaystyle\\mathbb{E}[(X\\!-\\!\\mu)^2]=\\int (x\\!-\\!\\mu)^2\\ p(x)\\,dx \\approx \\frac{1}{N}\\sum_i (x_i\\!-\\!\\mu)^2$\n",
    "><br/><br/>\n",
    "> compare to\n",
    "><br/><br/>\n",
    ">$\\displaystyle\\ s^2=\\frac{1}{N\\!-\\!1}\\sum_{i=1}^N \\big(x_i\\!-\\!\\bar{x}\\big)^2 $\n",
    "><br/><br/>\n",
    "> Bessel correction: $N\\!-\\!1$ independent $(x_i\\!-\\!\\bar{x})$ differences\n",
    "<br/><br/>\n",
    ">$\\displaystyle \\sum_{i=1}^N (x_i\\!-\\!\\bar{x}) =\\ ???$ <font color=\"white\">.... 0 ...</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm as gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# generate N samples\n",
    "mu, sigma, N = 0, 1, 5\n",
    "x = gaussian.rvs(mu, sigma, N)\n",
    "\n",
    "avg = np.mean(x)\n",
    "print (avg)\n",
    "\n",
    "# variance estimates\n",
    "s2   = np.sum( (x-avg)**2 ) /(N-1)  # correct\n",
    "s2n  = np.sum( (x-avg)**2 ) / N     # biased \n",
    "s2k  = np.sum( (x- mu)**2 ) / N     # known mean\n",
    "# standard deviation estimates\n",
    "np.sqrt(s2), np.sqrt(s2n), np.sqrt(s2k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# generate M runs with N samples each\n",
    "mu, sigma, N, M = 0, 1, 5, 100000\n",
    "X = gaussian.rvs(loc=mu, scale=sigma, size=(N,M))\n",
    "avg = np.mean(X, axis=0)\n",
    "print (X.shape, avg.shape)\n",
    "\n",
    "# variance estimates - check out broadcasting in X-avg\n",
    "s2   = np.sum( (X-avg)**2, axis=0) /(N-1) # correct\n",
    "s2n  = np.sum( (X-avg)**2, axis=0) / N    # biased\n",
    "s2k  = np.sum( (X- mu)**2, axis=0) / N    # known mean\n",
    "\n",
    "print (s2.shape)\n",
    "\n",
    "# standard deviation estimates\n",
    "s, sn, sk = np.sqrt(s2), np.sqrt(s2n), np.sqrt(s2k)\n",
    "print (s.mean(), sn.mean(), sk.mean())\n",
    "\n",
    "plt.hist(s , 101, range=[0,3], color='r', alpha=0.5);\n",
    "plt.hist(sn, 101, range=[0,3], color='b', alpha=0.5);\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Density Estimation\n",
    "- Histograms\n",
    "    - Width of bins, $h$\n",
    "    - Start of bin boundary, $x_0$\n",
    "\n",
    " >$\\displaystyle \\mathrm{Hist}(x) = \\frac{1}{N}\\sum_i \\pmb{1}_{\\mathrm{bin}(x_i;x_0,h)}(x)$\n",
    "        \n",
    "- Kernel Density Estimation (KDE)\n",
    "    - Bandwidth $h$\n",
    "   \n",
    " >$\\displaystyle \\mathrm{KDE}(x) = \\frac{1}{N}\\sum_i K_h(x\\!-\\!x_i) = \\frac{1}{Nh}\\sum_i K\\left(\\frac{x\\!-\\!x_i}{h}\\right)$\n",
    "   \n",
    "    - Can use different $K(\\cdot)$ kernel functions\n",
    "        - E.g., Uniform, Triangular, Gauss, Epanechnikov\n",
    "\n",
    "See animations at\n",
    "http://www.mglerner.com/blog/?p=28\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Kernel Function\n",
    "- Finite vs Infinite support\n",
    "- Numerical evaluations\n",
    "- Frequently used kernels\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/47/Kernels.svg\" alt=\"All of the above kernels in a common coordinate system\"  width=\"350\" align=left>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Learn more about KDE \n",
    "[here](https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/) and also check out Bayesian Blocks \n",
    "[here](https://jakevdp.github.io/blog/2012/09/12/dynamic-programming-in-python/)\n",
    "<br>\n",
    "&mdash; tutorials by Jake Vanderplas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Detour: Dirac delta\n",
    "\n",
    "- In the limit of $h\\rightarrow{}0$, the kernel will become strange:\n",
    "\n",
    "<img src=\"files/488px-Dirac_distribution_PDF.svg.png\" align=right width=250>\n",
    "\n",
    "> **Dirac's $\\delta$** \"function\" is 0 everywhere except at 0 such that\n",
    "<br/>\n",
    "> $\\displaystyle \\int \\delta(x)\\,dx = 1$\n",
    "\n",
    "- Interesting properties, e.g., \n",
    "\n",
    "> $\\displaystyle \\int f(x)\\,\\delta(x\\!-\\!a)\\,dx = f(a)$\n",
    "\n",
    "-  See **distribution theory** and **functionals** for more background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### An interesting result \n",
    "\n",
    "- Bad density estimation but if...\n",
    "\n",
    "> $\\displaystyle p(x) = \\frac{1}{N} \\sum_{i=1}^N \\delta(x\\!-\\!x_i)$\n",
    "\n",
    "- The expectation value\n",
    "\n",
    "> $\\displaystyle \\mathbb{E}[X] =  \\int x\\, \\frac{1}{N} \\sum_{i=1}^N  \\delta(x\\!-\\!x_i) \\,dx$\n",
    "> <br/><br/>\n",
    "> $\\displaystyle \\mathbb{E}[X] = \\frac{1}{N} \\sum_{i=1}^N \\int x\\, \\delta(x\\!-\\!x_i) \\,dx$\n",
    "><br/><br/>\n",
    "> $\\displaystyle \\mathbb{E}[X] = \\frac{1}{N} \\sum_{i=1}^N x_i$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Unhomework\n",
    "\n",
    "1. Sample from a mixture of two Gaussians using uniform random numbers in the [0,1) interval. Try different $(\\mu_1, \\sigma_1)$ and $(\\mu_2,\\sigma_2$) values and different $w_1$ and $w_2$ weights!\n",
    "1. Build different density estimators and compare to the original PDF. Try histogramming and KDE with different parameters."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}